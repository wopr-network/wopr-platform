# =============================================================================
# GPU Inference Stack Configuration
# =============================================================================
# Copy and customize: cp .env.gpu.example .env.gpu
# Run: docker compose -f docker-compose.gpu.yml --env-file .env.gpu up -d
#
# Prerequisites:
#   - NVIDIA Container Toolkit: https://github.com/NVIDIA/nvidia-container-toolkit
#   - GPU drivers installed and working (nvidia-smi should show your GPUs)
#   - Model weights downloaded to MODELS_PATH directory
#
# =============================================================================
# REQUIRED MODEL FILES (MUST BE DOWNLOADED BEFORE FIRST START)
# =============================================================================
# Download these files and place them in ${MODELS_PATH} (default: ./models):
#
# 1. LLAMA_MODEL_FILE (text generation):
#    - Download from: https://huggingface.co/models?library=gguf&search=llama
#    - Recommended: Meta-Llama-3.1-8B-Instruct-Q8_0.gguf (~9 GB)
#    - Path: ./models/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf
#
# 2. EMBEDDINGS_MODEL_FILE (embeddings):
#    - Download from: https://huggingface.co/Qwen/Qwen2-0.5B-Instruct-GGUF
#    - Recommended: qwen2-0_5b-instruct-q8_0.gguf (~2 GB)
#    - Path: ./models/qwen2-0_5b-instruct-q8_0.gguf
#
# 3. WHISPER_MODEL (speech-to-text):
#    - Auto-downloaded from HuggingFace on first container start
#    - No manual download required
#
# 4. Chatterbox TTS models:
#    - Bundled in the Docker image
#    - No manual download required
#
# WARNING: If model files are missing, llama-cpp and qwen-embeddings containers
# will crash-loop with "failed to load model" errors. Verify files exist before
# running docker compose up.
# =============================================================================

# --- Model Weights ---
# Directory on the host containing GGUF model files.
# Download models from HuggingFace and place them here.
MODELS_PATH=./models

# --- llama.cpp (text generation) ---
# Example: Meta-Llama-3.1-8B-Instruct-Q8_0.gguf (9 GB VRAM)
#          Meta-Llama-3.1-70B-Instruct-Q4_K_M.gguf (40 GB VRAM)
LLAMA_MODEL_FILE=Meta-Llama-3.1-8B-Instruct-Q8_0.gguf
LLAMA_CTX_SIZE=4096
# -1 means all layers on GPU (fastest). Reduce if VRAM is limited.
LLAMA_GPU_LAYERS=-1
# Number of parallel request slots (increase for higher throughput)
LLAMA_PARALLEL=4

# --- Qwen Embeddings ---
# Example: qwen2-0_5b-instruct-q8_0.gguf (2 GB VRAM)
EMBEDDINGS_MODEL_FILE=qwen2-0_5b-instruct-q8_0.gguf
EMBEDDINGS_CTX_SIZE=8192
# Pooling strategy for sentence embeddings (mean, cls, last)
EMBEDDINGS_POOLING=mean

# --- Chatterbox TTS ---
# Optional: Directory containing voice cloning samples
CHATTERBOX_VOICES_PATH=./voices

# --- Whisper STT ---
# Auto-downloaded from HuggingFace on first start
# Options: Systran/faster-whisper-tiny (1 GB VRAM)
#          Systran/faster-whisper-small (1 GB VRAM)
#          Systran/faster-whisper-medium (2 GB VRAM)
#          Systran/faster-whisper-large-v3 (4 GB VRAM)
WHISPER_MODEL=Systran/faster-whisper-small

# --- VRAM Budget Summary (default config) ---
# llama-cpp:         ~9 GB  (Llama 3.1 8B Q8_0)
# qwen-embeddings:   ~2 GB  (Qwen2-0.5B)
# chatterbox:        ~5 GB  (Chatterbox TTS)
# whisper:           ~1 GB  (faster-whisper-small)
# -------------------------------------------------
# TOTAL:            ~17 GB  (leaves ~63 GB on A100 80GB)

# --- Multi-GPU Configuration ---
# To split across multiple GPUs, create a docker-compose.gpu.override.yml:
#
# services:
#   llama-cpp:
#     deploy:
#       resources:
#         reservations:
#           devices:
#             - driver: nvidia
#               device_ids: ['0']
#               capabilities: [gpu]
#
#   qwen-embeddings:
#     deploy:
#       resources:
#         reservations:
#           devices:
#             - driver: nvidia
#               device_ids: ['0']
#               capabilities: [gpu]
#
#   chatterbox:
#     deploy:
#       resources:
#         reservations:
#           devices:
#             - driver: nvidia
#               device_ids: ['1']
#               capabilities: [gpu]
#
#   whisper:
#     deploy:
#       resources:
#         reservations:
#           devices:
#             - driver: nvidia
#               device_ids: ['1']
#               capabilities: [gpu]
#
# Then run: docker compose -f docker-compose.gpu.yml -f docker-compose.gpu.override.yml up -d

# --- Gateway Integration (Future: WOP-506) ---
# Once WOP-506 wraps Chatterbox/Whisper in OpenAI-compatible APIs,
# you can point the WOPR gateway at the local inference stack:
#
# OPENROUTER_BASE_URL=http://llama-cpp:8080
# ELEVENLABS_BASE_URL=http://chatterbox:8081
# DEEPGRAM_BASE_URL=http://whisper:8082
#
# This replaces OpenRouter/ElevenLabs/Deepgram cloud APIs with self-hosted inference.
