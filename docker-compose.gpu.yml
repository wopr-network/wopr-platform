## GPU Inference Stack for WOPR Platform
##
## Run:
##   docker compose -f docker-compose.gpu.yml --env-file .env.gpu up -d
##
## Prerequisites:
##   - NVIDIA Container Toolkit installed (https://github.com/NVIDIA/nvidia-container-toolkit)
##   - GPU drivers configured and working
##   - Copy .env.gpu.example to .env.gpu and customize
##   - Download model weights to ${MODELS_PATH} (see .env.gpu.example for required files)
##
## This spins up 4 GPU-accelerated inference services:
##   - llama-cpp: Text generation (OpenAI-compatible /v1/chat/completions)
##   - qwen-embeddings: Embedding generation (OpenAI-compatible /v1/embeddings)
##   - chatterbox: Text-to-speech (OpenAI-compatible /v1/audio/speech)
##   - whisper: Speech-to-text (OpenAI-compatible /v1/audio/transcriptions)

x-gpu-common: &gpu-common
  deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            count: 1
            capabilities: [gpu]

services:
  llama-cpp:
    <<: *gpu-common
    image: ghcr.io/ggerganov/llama.cpp:server-cuda
    container_name: wopr-llama-cpp
    restart: unless-stopped
    env_file: .env.gpu
    ports:
      - "8080:8080"
    volumes:
      - ${MODELS_PATH:-./models}:/models:ro
    command:
      - --host
      - "0.0.0.0"
      - --port
      - "8080"
      - --model
      - /models/${LLAMA_MODEL_FILE}
      - --ctx-size
      - "${LLAMA_CTX_SIZE:-4096}"
      - --n-gpu-layers
      - "${LLAMA_GPU_LAYERS:--1}"
      - --parallel
      - "${LLAMA_PARALLEL:-4}"
      - --cont-batching
      - --flash-attn
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      start_period: 120s
      retries: 3
    networks:
      - wopr-gpu

  qwen-embeddings:
    <<: *gpu-common
    image: ghcr.io/ggerganov/llama.cpp:server-cuda
    container_name: wopr-qwen-embeddings
    restart: unless-stopped
    env_file: .env.gpu
    ports:
      - "8083:8080"
    volumes:
      - ${MODELS_PATH:-./models}:/models:ro
    command:
      - --host
      - "0.0.0.0"
      - --port
      - "8080"
      - --model
      - /models/${EMBEDDINGS_MODEL_FILE}
      - --ctx-size
      - "${EMBEDDINGS_CTX_SIZE:-8192}"
      - --n-gpu-layers
      - "-1"
      - --embedding
      - --pooling
      - "${EMBEDDINGS_POOLING:-mean}"
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      start_period: 120s
      retries: 3
    networks:
      - wopr-gpu

  chatterbox:
    <<: *gpu-common
    image: travisvn/chatterbox-tts-api:latest
    container_name: wopr-chatterbox
    restart: unless-stopped
    env_file: .env.gpu
    ports:
      - "8081:5123"
    environment:
      CHATTERBOX_DEVICE: cuda
    volumes:
      - ${CHATTERBOX_VOICES_PATH:-./voices}:/app/voices:ro
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:5123/health || curl -sf http://localhost:5123/"]
      interval: 30s
      timeout: 10s
      start_period: 120s
      retries: 3
    networks:
      - wopr-gpu

  whisper:
    <<: *gpu-common
    image: fedirz/faster-whisper-server:latest-cuda
    container_name: wopr-whisper
    restart: unless-stopped
    env_file: .env.gpu
    ports:
      - "8082:8000"
    environment:
      WHISPER__MODEL: ${WHISPER_MODEL:-Systran/faster-whisper-small}
      WHISPER__INFERENCE_DEVICE: cuda
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      start_period: 120s
      retries: 3
    networks:
      - wopr-gpu

networks:
  wopr-gpu:
    name: wopr-gpu
    driver: bridge
